{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_prompt_path = \"evaluator.prompty\"\n",
    "\n",
    "input_data = \"data\\input_data\\evaluator_alignment_data.jsonl\"\n",
    "\n",
    "self_improver_prompt = \"self_improver.prompty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from evaluator import evaluator\n",
    "from azure.ai.evaluation import evaluate\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import prompty.azure\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the prompt\n",
    "def get_eval(prompt_path, data_path):\n",
    "    evaluate_results = evaluate(\n",
    "        data=data_path,\n",
    "        evaluators={\n",
    "            \"eval\": evaluator\n",
    "        },\n",
    "        evaluator_config={\n",
    "            \"default\": {\n",
    "                \"question\": \"${data.question}\",\n",
    "                \"answer\": \"${data.answer}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"prompt_path\": prompt_path,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    eval_result = pd.DataFrame(evaluate_results[\"rows\"])\n",
    "\n",
    "    # Extract 'chain of thought' from the JSON strings\n",
    "    eval_result['chain of thought'] = eval_result['outputs.eval.output'].apply(lambda x: json.loads(x)['chain of thought'])\n",
    "\n",
    "    # Extract 'following guidelines'\n",
    "    eval_result['following guidelines'] = eval_result['outputs.eval.output'].apply(lambda x: json.loads(x)['following guidelines'])\n",
    "\n",
    "    return eval_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_results(results, number_of_correct_samples=5, number_of_incorrect_samples=5):\n",
    "    correct_samples = results[results['correct evaluated'] == True].sample(number_of_correct_samples)\n",
    "    incorrect_samples = results[results['correct evaluated'] == False].sample(number_of_incorrect_samples)\n",
    "    \n",
    "    samples = pd.concat([correct_samples, incorrect_samples])\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "#\"azure_endpoint\": os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "#\"api_version\": os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "\"api_key\": os.environ[\"AZURE_OPENAI_KEY\"]\n",
    "}\n",
    "\n",
    "# Construct the absolute path\n",
    "prompty_file_path = os.path.join(os.getcwd(), 'self_improver.prompty')\n",
    "\n",
    "def sample_new_prompts(prompt_to_expand, results, number=2):\n",
    "\n",
    "    mutated_prompts = []\n",
    "    for i in range(0, number):\n",
    "          # execute the prompty file\n",
    "      result = prompty.execute(\n",
    "        prompty_file_path, \n",
    "        inputs={\n",
    "          \"prompt\": prompt_to_expand,\n",
    "          \"example_results\": sample_results(results, 1, 1)\n",
    "        },\n",
    "        configuration=model_config\n",
    "      )\n",
    "\n",
    "      result_dict = json.loads(result)\n",
    "\n",
    "      mutated_prompts.append(result_dict['new prompt'])\n",
    "\n",
    "    return mutated_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_to_human_labels(results, human_labels):\n",
    "    results['human_label'] = human_labels\n",
    "    results['correct evaluated'] = results['following guidelines'] == results['human_label']\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(results):\n",
    "    return roc_auc_score(results['human_label'], results['following guidelines'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. Evaluate seed prompt\n",
    "\n",
    "# 1. Calculate AUC of seed prompt\n",
    "\n",
    "# 2. Generate new prompts\n",
    "\n",
    "# 3. Evaluate new prompts\n",
    "\n",
    "# 4. Calculate UCT\n",
    "\n",
    "# 5. Select prompt to expand\n",
    "\n",
    "# 6. Iterate 2-5 until stopping criteria is met"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_labels(input_data):\n",
    "    human_labels = []\n",
    "    with open(input_data, 'r') as file:\n",
    "        for line in file:\n",
    "            json_obj = json.loads(line.strip())\n",
    "            human_label = json_obj[\"human_label\"].strip().lower() == 'true'\n",
    "            human_labels.append(human_label)\n",
    "    return human_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class MCTSNode:\n",
    "    def __init__(self, prompt, value, results_benchmarked, parent=None):\n",
    "        self.prompt = prompt\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = value\n",
    "        self.results_benchmarked = results_benchmarked\n",
    "\n",
    "    def add_child(self, child):\n",
    "        self.children.append(child)\n",
    "\n",
    "\n",
    "class MCTSTree:\n",
    "    def __init__(self, input_data, root_prompt, value, results_benchmarked, max_expansions=2):\n",
    "        self.root = MCTSNode(prompt=root_prompt, value=value, results_benchmarked=results_benchmarked)\n",
    "        self.input_data = input_data\n",
    "        self.human_labels = get_human_labels(input_data)\n",
    "        self.max_expansions = max_expansions\n",
    "        self.expansions= 0\n",
    "\n",
    "    def add_child(self, parent_node, child_prompt, value):\n",
    "        child_node = MCTSNode(child_prompt, parent=parent_node, value=value)\n",
    "        parent_node.add_child(child_node)\n",
    "        return child_node\n",
    "    \n",
    "    def select_node(self):\n",
    "        current_node = self.root\n",
    "        nodes_traversed = [current_node]\n",
    "        # traverse down until a leaf node\n",
    "        while current_node.children:\n",
    "            current_node = self._uct_select(current_node)\n",
    "            nodes_traversed.append(current_node)\n",
    "        # Add a visit to all parent nodes to the selected leaf node\n",
    "        for node in nodes_traversed:\n",
    "            node.visits += 1\n",
    "        return current_node\n",
    "    \n",
    "    def _uct_select(node, c=1.41):\n",
    "        best_child = None\n",
    "        best_uct_value = -float('inf')\n",
    "        for child in node.children:\n",
    "            # If child not explored yet, pick it immediately\n",
    "            if child.visits == 0:\n",
    "                return child\n",
    "            exploitation = child.value / child.visits\n",
    "            exploration = c * math.sqrt(math.log(node.visits + 1) / (child.visits + 1))\n",
    "            uct_value = exploitation + exploration\n",
    "            if uct_value > best_uct_value:\n",
    "                best_uct_value = uct_value\n",
    "                best_child = child\n",
    "        return best_child\n",
    "    \n",
    "    def expand_node(self, node):\n",
    "        # Generate new prompts\n",
    "        new_prompts = sample_new_prompts(node.prompt, node.results_benchmarked, number=2)\n",
    "        for prompt in new_prompts:\n",
    "            \n",
    "            print(prompt)\n",
    "\n",
    "            result = get_eval(prompt['new prompt'], input_data)\n",
    "\n",
    "            result_benchmarked = compare_to_human_labels(result, self.human_labels)\n",
    "\n",
    "            auc = get_auc(result_benchmarked)\n",
    "\n",
    "            leaf_node = MCTSNode(prompt, parent=node, value=auc, results_benchmarked=result_benchmarked)\n",
    "            \n",
    "            self.add_child(node, leaf_node)\n",
    "    \n",
    "    def expand_tree(self):\n",
    "        node = self.select_node()\n",
    "        self.expand_node(node)\n",
    "\n",
    "    def _find_best_node(self):\n",
    "        best_node = self.root\n",
    "        queue = [self.root]\n",
    "        while queue:\n",
    "            current = queue.pop(0)\n",
    "            if current.value > best_node.value:\n",
    "                best_node = current\n",
    "            queue.extend(current.children)\n",
    "        return best_node\n",
    "    \n",
    "    def run_mcts(self):\n",
    "        while self.expansions < self.max_expansions:\n",
    "            self.expand_tree()\n",
    "            self.expansions += 1\n",
    "        \n",
    "        return self._find_best_node()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-22 15:50:29 +0100][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2024-12-22 15:50:29 +0100][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run evaluator_evaluator_b0n0q2c8_20241222_155029_314444, log path: C:\\Users\\albinlnnflt\\.promptflow\\.runs\\evaluator_evaluator_b0n0q2c8_20241222_155029_314444\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "You can view the traces in local from http://127.0.0.1:23333/v1.0/ui/traces/?#run=evaluator_evaluator_b0n0q2c8_20241222_155029_314444\n",
      "2024-12-22 15:50:29 +0100   33264 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2024-12-22 15:50:29 +0100   33264 execution.bulk     INFO     The timeout for the batch run is 3600 seconds.\n",
      "2024-12-22 15:50:29 +0100   33264 execution.bulk     INFO     Current system's available memory is 10275.921875MB, memory consumption of current process is 821.296875MB, estimated available worker count is 10275.921875/821.296875 = 12\n",
      "2024-12-22 15:50:29 +0100   33264 execution.bulk     INFO     Set process count to 4 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 8, 'estimated_worker_count_based_on_memory_usage': 12}.\n",
      "2024-12-22 15:50:33 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(0) start execution.\n",
      "2024-12-22 15:50:33 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-9)-Process id(36192)-Line number(1) start execution.\n",
      "2024-12-22 15:50:33 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-8)-Process id(39816)-Line number(2) start execution.\n",
      "2024-12-22 15:50:33 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(7976)-Line number(3) start execution.\n",
      "2024-12-22 15:50:38 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(0) completed.\n",
      "2024-12-22 15:50:38 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(4) start execution.\n",
      "2024-12-22 15:50:39 +0100   33264 execution.bulk     INFO     Finished 1 / 8 lines.\n",
      "2024-12-22 15:50:39 +0100   33264 execution.bulk     INFO     Average execution time for completed lines: 6.05 seconds. Estimated time for incomplete lines: 42.35 seconds.\n",
      "2024-12-22 15:50:40 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(4) completed.\n",
      "2024-12-22 15:50:40 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(5) start execution.\n",
      "2024-12-22 15:50:40 +0100   33264 execution.bulk     INFO     Finished 2 / 8 lines.\n",
      "2024-12-22 15:50:40 +0100   33264 execution.bulk     INFO     Average execution time for completed lines: 3.53 seconds. Estimated time for incomplete lines: 21.18 seconds.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(5) completed.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(6) start execution.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Finished 3 / 8 lines.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Average execution time for completed lines: 3.02 seconds. Estimated time for incomplete lines: 15.1 seconds.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(7976)-Line number(3) completed.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(7976)-Line number(7) start execution.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-9)-Process id(36192)-Line number(1) completed.\n",
      "2024-12-22 15:50:42 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-8)-Process id(39816)-Line number(2) completed.\n",
      "2024-12-22 15:50:43 +0100   33264 execution.bulk     INFO     Finished 6 / 8 lines.\n",
      "2024-12-22 15:50:43 +0100   33264 execution.bulk     INFO     Average execution time for completed lines: 1.68 seconds. Estimated time for incomplete lines: 3.36 seconds.\n",
      "2024-12-22 15:50:43 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-7)-Process id(26652)-Line number(6) completed.\n",
      "2024-12-22 15:50:44 +0100   33264 execution.bulk     INFO     Finished 7 / 8 lines.\n",
      "2024-12-22 15:50:44 +0100   33264 execution.bulk     INFO     Average execution time for completed lines: 1.58 seconds. Estimated time for incomplete lines: 1.58 seconds.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     Process name(SpawnProcess-10)-Process id(7976)-Line number(7) completed.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     Finished 8 / 8 lines.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     Average execution time for completed lines: 1.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     The thread monitoring the process [36192-SpawnProcess-9] will be terminated.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     The thread monitoring the process [39816-SpawnProcess-8] will be terminated.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     The thread monitoring the process [26652-SpawnProcess-7] will be terminated.\n",
      "2024-12-22 15:50:47 +0100   33264 execution.bulk     INFO     The thread monitoring the process [7976-SpawnProcess-10] will be terminated.\n",
      "2024-12-22 15:50:47 +0100   36192 execution.bulk     INFO     The process [36192] has received a terminate signal.\n",
      "2024-12-22 15:50:47 +0100   39816 execution.bulk     INFO     The process [39816] has received a terminate signal.\n",
      "2024-12-22 15:50:47 +0100   26652 execution.bulk     INFO     The process [26652] has received a terminate signal.\n",
      "2024-12-22 15:50:47 +0100    7976 execution.bulk     INFO     The process [7976] has received a terminate signal.\n",
      "2024-12-22 15:50:48 +0100   33264 execution.bulk     INFO     Process 36192 terminated.\n",
      "2024-12-22 15:50:48 +0100   33264 execution.bulk     INFO     Process 26652 terminated.\n",
      "2024-12-22 15:50:48 +0100   33264 execution.bulk     INFO     Process 7976 terminated.\n",
      "2024-12-22 15:50:48 +0100   33264 execution.bulk     INFO     Process 39816 terminated.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"evaluator_evaluator_b0n0q2c8_20241222_155029_314444\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-12-22 15:50:29.310845+01:00\"\n",
      "Duration: \"0:00:18.982885\"\n",
      "Output path: \"C:\\Users\\albinlnnflt\\.promptflow\\.runs\\evaluator_evaluator_b0n0q2c8_20241222_155029_314444\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"eval\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:18.982885\",\n",
      "        \"completed_lines\": 8,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\albinlnnflt\\\\.promptflow\\\\.runs\\\\evaluator_evaluator_b0n0q2c8_20241222_155029_314444\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = get_eval(seed_prompt_path, input_data)\n",
    "\n",
    "human_labels = get_human_labels(input_data)\n",
    "\n",
    "result_benchmarked = compare_to_human_labels(result, human_labels)\n",
    "\n",
    "auc = get_auc(result_benchmarked)\n",
    "\n",
    "mctsTree = MCTSTree(\n",
    "                input_data=input_data,\n",
    "                root_prompt=seed_prompt_path,\n",
    "                value=auc,\n",
    "                results_benchmarked=result_benchmarked,\n",
    "                max_expansions=2\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator.prompty \n",
      "\n",
      " Ensure that the chatbot's response is polite, clear, and informative. Focus specifically on maintaining a neutral tone while providing factual and accurate information. Responses should be concise and directly address the user's question. Double-check for any compliance with communication guidelines to bolster user confidence in the chatbot's responses.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_prompt \u001b[38;5;241m=\u001b[39m \u001b[43mmctsTree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_mcts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 89\u001b[0m, in \u001b[0;36mMCTSTree.run_mcts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_mcts\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpansions \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_expansions:\n\u001b[1;32m---> 89\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpansions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_best_node()\n",
      "Cell \u001b[1;32mIn[22], line 75\u001b[0m, in \u001b[0;36mMCTSTree.expand_tree\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexpand_tree\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     74\u001b[0m     node \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_node()\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 63\u001b[0m, in \u001b[0;36mMCTSTree.expand_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m new_prompts:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(prompt)\n\u001b[1;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m get_eval(\u001b[43mprompt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnew prompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, input_data)\n\u001b[0;32m     65\u001b[0m     result_benchmarked \u001b[38;5;241m=\u001b[39m compare_to_human_labels(result, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhuman_labels)\n\u001b[0;32m     67\u001b[0m     auc \u001b[38;5;241m=\u001b[39m get_auc(result_benchmarked)\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "best_prompt = mctsTree.run_mcts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_alignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
